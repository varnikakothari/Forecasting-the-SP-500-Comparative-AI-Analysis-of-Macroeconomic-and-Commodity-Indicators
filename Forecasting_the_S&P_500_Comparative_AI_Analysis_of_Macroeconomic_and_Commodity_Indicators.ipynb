{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varnikakothari/Forecasting-the-SP-500-Comparative-AI-Analysis-of-Macroeconomic-and-Commodity-Indicators/blob/main/Forecasting_the_S%26P_500_Comparative_AI_Analysis_of_Macroeconomic_and_Commodity_Indicators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forecasting the S&P 500: A Comparative Analysis of Traditional and Non-Linear Macroeconomic Indicators and Commodity Linkages Using Artificial Intelligence Models\n",
        "\n",
        "Varnika Kothari, Rami Abi Akl"
      ],
      "metadata": {
        "id": "MRdoMbtCg3jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datasets Used:\n",
        "\n",
        "US Stock Market Data: S&P 500 Index (1901-2025):\n",
        "Author: Ahmadul Karim Chowdhury\n",
        "https://www.kaggle.com/datasets/ahmadulkc/s-and-p-500-historical-monthly-prices-19012025\n",
        "\n",
        "Crude Oil Price (1983-Present):\n",
        "Author: Chandan Singh\n",
        "License: CCO: Public Domain\n",
        "https://www.kaggle.com/datasets/sc231997/crude-oil-price\n",
        "\n",
        "Gold, Silver & Precious Metals Futures (2000-2024):\n",
        "Author: Guillem SD\n",
        "License: Attribution-NonCommercial 4.0 International\n",
        "https://www.kaggle.com/datasets/guillemservera/precious-metals-data\n",
        "\n",
        "USA Macroeconomic Rate of Changes (1993-2025):\n",
        "Data collected from FRED-Federal Reserve Economic Data from the Bank of St.Louis\n",
        "https://fred.stlouisfed.org/\n",
        "Compiled Dataset:- https://docs.google.com/spreadsheets/d/10q3gGOYpTtAfJsDukJU2x3SYuyGlvYIq/edit?usp=sharing&ouid=117010112379515535428&rtpof=true&sd=true\n",
        "\n",
        "\n",
        "*Note that all model results print MSE that is Mean Squared Error which is then mathematically square rooted to obtain RMSE or Root Mean Squared Error"
      ],
      "metadata": {
        "id": "i2AfYgaAj2ay"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L-0Z_wdYT_z"
      },
      "source": [
        "#USA S&P 500 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYFv02gsfrIW"
      },
      "outputs": [],
      "source": [
        "#To import the dataset and create a new one with only data after 1993\n",
        "\n",
        "import pandas as pd\n",
        "import kagglehub\n",
        "import datetime\n",
        "\n",
        "FILE_PATH = \"S_P 500 Historical Data.csv\"\n",
        "NEW_FILE_NAME = \"SP500_1993_onwards.csv\"\n",
        "START_DATE_FILTER = '1993-01-01'\n",
        "\n",
        "df_SP = kagglehub.load_dataset(\n",
        "    kagglehub.KaggleDatasetAdapter.PANDAS,\n",
        "    \"ahmadulkc/s-and-p-500-historical-monthly-prices-19012025\",\n",
        "    FILE_PATH,\n",
        "    pandas_kwargs={\n",
        "        'thousands': ',',\n",
        "    }\n",
        ")\n",
        "df_SP['Date'] = pd.to_datetime(df_SP['Date'], format='%m-%d-%y', errors='coerce')\n",
        "\n",
        "df_SP['Date'] = df_SP['Date'].apply(lambda x: x.replace(year=x.year - 100)\n",
        "                                    if pd.notna(x) and x.year < 1993\n",
        "                                    else x)\n",
        "\n",
        "df_SP['Change %'] = df_SP['Change %'].astype(str).str.replace('%', '', regex=False)\n",
        "df_SP['Change %'] = pd.to_numeric(df_SP['Change %'], errors='coerce') / 100\n",
        "\n",
        "numeric_cols_to_check = ['Price', 'Open', 'High', 'Low']\n",
        "for col in numeric_cols_to_check:\n",
        "    if col in df_SP.columns and df_SP[col].dtype == 'object':\n",
        "        df_SP[col] = pd.to_numeric(df_SP[col], errors='coerce')\n",
        "\n",
        "df_1993_onwards = df_SP[df_SP['Date'] >= START_DATE_FILTER].copy()\n",
        "\n",
        "df_1993_onwards = df_1993_onwards.drop(columns=['Open', 'High','Low'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LZrTbO8o0fA"
      },
      "outputs": [],
      "source": [
        "df_1993_onwards['Date'] = pd.to_datetime(df_1993_onwards['Date'])\n",
        "\n",
        "df_1993_onwards['Year'] = df_1993_onwards['Date'].dt.year\n",
        "df_1993_onwards['Month'] = df_1993_onwards['Date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llTAf20oo4Cg"
      },
      "outputs": [],
      "source": [
        "df_1993_onwards.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv3EI8s9aEuy"
      },
      "source": [
        "#Crude Oil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3LmqcaUZ8Rp"
      },
      "outputs": [],
      "source": [
        "#To import the dataset\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"crude-oil-price.csv\"\n",
        "\n",
        "df_Crude = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"sc231997/crude-oil-price\",\n",
        "  file_path,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jGwSmkRlQkj"
      },
      "source": [
        "#Gold, Silver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TD5L-q9WlNRR"
      },
      "outputs": [],
      "source": [
        "#To import the dataset\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"all_commodities_data.csv\"\n",
        "df_commodity = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"guillemservera/precious-metals-data\",\n",
        "  file_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_commodity.head()"
      ],
      "metadata": {
        "id": "qln2h8Us0g3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQVMb_KliIbD"
      },
      "source": [
        "#Macroeconomic Indicators\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Please download Macro.xlsx using the link given at the beginning and save it in My Drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "df_indicators = pd.read_excel(\"/content/drive/My Drive/Macro.xlsx\")\n"
      ],
      "metadata": {
        "id": "NJxNG7cj1fq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_indicators['Year'] = df_indicators['observation_date'].dt.year\n",
        "df_indicators['Month'] = df_indicators['observation_date'].dt.month\n",
        "\n",
        "df_indicators.head()"
      ],
      "metadata": {
        "id": "XDWPrEza2inr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_indicators.head()"
      ],
      "metadata": {
        "id": "k5Rrho9p0rPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyf-QvKUmCPY"
      },
      "source": [
        "#Final Data Set Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX43-Z6ThowR"
      },
      "outputs": [],
      "source": [
        "#The dataset created in this code bloc is only used for testing the RNN Models and is simply a theoretical representation as it assumes SP500 Prices remain constant throughout a month\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# --- Preprocessing for df_1993_onwards (SP500 data) ---\n",
        "df_1993_onwards['Date'] = pd.to_datetime(df_1993_onwards['Date'])\n",
        "df_1993_onwards['Year'] = df_1993_onwards['Date'].dt.year\n",
        "df_1993_onwards['Month'] = df_1993_onwards['Date'].dt.month\n",
        "# Rename the 'Price' column for SP500\n",
        "df_1993_onwards = df_1993_onwards.rename(columns={'Price': 'SP500_Price'})\n",
        "\n",
        "\n",
        "# --- Preprocessing for df_Crude ---\n",
        "df_Crude['date'] = pd.to_datetime(df_Crude['date'])\n",
        "df_Crude['Year'] = df_Crude['date'].dt.year\n",
        "df_Crude['Month'] = df_Crude['date'].dt.month\n",
        "# Rename the 'price' column for Crude (optional, but good for consistency)\n",
        "df_Crude = df_Crude.rename(columns={'price': 'Crude_Price'})\n",
        "\n",
        "\n",
        "# --- Preprocessing for df_commodity (Gold data) ---\n",
        "# 1. Filter for 'Gold' commodity\n",
        "df_gold_commodity = df_commodity[df_commodity['commodity'] == 'Gold'].copy()\n",
        "df_silver_commodity = df_commodity[df_commodity['commodity'] == 'Silver'].copy()\n",
        "df_gold_commodity = df_gold_commodity.rename(columns={'close': 'Gold_Price'})\n",
        "df_silver_commodity = df_silver_commodity.rename(columns={'close': 'Silver_Price'})\n",
        "# 2. Convert 'date' to datetime and extract 'Year' and 'Month'\n",
        "df_gold_commodity['date'] = pd.to_datetime(df_gold_commodity['date'])\n",
        "df_gold_commodity['Year'] = df_gold_commodity['date'].dt.year\n",
        "df_gold_commodity['Month'] = df_gold_commodity['date'].dt.month\n",
        "df_silver_commodity['date'] = pd.to_datetime(df_silver_commodity['date'])\n",
        "df_silver_commodity['Year'] = df_silver_commodity['date'].dt.year\n",
        "df_silver_commodity['Month'] = df_silver_commodity['date'].dt.month\n",
        "\n",
        "# --- Perform sequential merges ---\n",
        "\n",
        "# First merge: df_1993_onwards (SP500) with df_indicators\n",
        "merged_df_step1 = pd.merge(df_1993_onwards, df_indicators, on=['Year', 'Month'], how='inner')\n",
        "\n",
        "# Second merge: result of step 1 with df_Crude\n",
        "merged_df_step2 = pd.merge(merged_df_step1, df_Crude, on=['Year', 'Month'], how='inner')\n",
        "\n",
        "# Final merge: result of step 2 with the filtered df_gold_commodity\n",
        "merged_df_step3 = pd.merge(merged_df_step2, df_gold_commodity, on=['Year', 'Month'], how='inner')\n",
        "merged_df = pd.merge(merged_df_step3, df_silver_commodity, on=['Year', 'Month'], how='inner')\n",
        "merged_df.head()\n",
        "\n",
        "merged_df['SP500_NextPrice']=merged_df['SP500_Price'].shift(-1)\n",
        "merged_df=merged_df.iloc[:-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.head()"
      ],
      "metadata": {
        "id": "ZW4OdhQINZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This is the dataset used for testing other models except RNN, and consists of SP500 Prices for the 1st date of every month.\n",
        "\n",
        "# --- Preprocessing for df_commodity (Gold data) ---\n",
        "\n",
        "# 2. Convert 'date' to datetime and extract 'Year' and 'Month'\n",
        "monthly_avg_gold = df_gold_commodity.groupby(df_gold_commodity[\"date\"].dt.to_period(\"M\"))[\"Gold_Price\"].mean().reset_index()\n",
        "monthly_avg_gold['Year'] = monthly_avg_gold['date'].dt.year\n",
        "monthly_avg_gold['Month'] = monthly_avg_gold['date'].dt.month\n",
        "monthly_avg_silver = df_silver_commodity.groupby(df_silver_commodity[\"date\"].dt.to_period(\"M\"))[\"Silver_Price\"].mean().reset_index()\n",
        "monthly_avg_silver['Year'] = monthly_avg_silver['date'].dt.year\n",
        "monthly_avg_silver['Month'] = monthly_avg_silver['date'].dt.month\n",
        "\n",
        "# --- Perform sequential merges ---\n",
        "# Final merge: result of step 2 with the filtered df_gold_commodity\n",
        "merged_df_small = pd.merge(merged_df_step2, monthly_avg_gold, on=['Year', 'Month'], how='inner')\n",
        "smallmerged_df = pd.merge(merged_df_small, monthly_avg_silver, on=['Year', 'Month'], how='inner')\n",
        "smallmerged_df['SP500_NextPrice']=smallmerged_df['SP500_Price'].shift(-1)\n",
        "smallmerged_df=smallmerged_df.iloc[:-1]\n",
        "smallmerged_df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wHi6z3ZoM1GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w0qTbkuZaRc"
      },
      "source": [
        "#Correlation Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdlhmK2K4CjE"
      },
      "outputs": [],
      "source": [
        "# --- 4. Select numeric columns for correlation ---\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df_numeric = smallmerged_df.select_dtypes(include=['number'])\n",
        "\n",
        "# --- 5. Calculate Correlation Matrix ---\n",
        "corr_matrix = df_numeric.corr()\n",
        "\n",
        "# --- 6. Define columns for the heatmap (explicitly include all desired columns) ---\n",
        "# Ensure these columns exist in your df_numeric after merging your actual data\n",
        "columns_for_heatmap = [\n",
        "    'SP500_Price',\n",
        "    'Crude_Price',\n",
        "    'Gold_Price',\n",
        "    'Silver_Price',\n",
        "    'Industrial_Production',\n",
        "    'Consumer_Price_Index',\n",
        "    'Unemployment_Rate',\n",
        "    'Retail_Sales',\n",
        "    'Producer_Price_Index',\n",
        "    'Personal_Consumption_Expenditures',\n",
        "    'National_Home_Price_Index',\n",
        "    'All_Employees(Total_Nonfarm)',\n",
        "    'Labor_Force_Participation_Rate',\n",
        "    'Money_Supply_(M2)',\n",
        "    'Personal_Income',\n",
        "    'Trade_Balance',\n",
        "    'Consumer_Sentiment',]\n",
        "\n",
        "# Filter the list to only include columns that actually exist in the correlation matrix\n",
        "existing_columns_for_heatmap = [col for col in columns_for_heatmap if col in corr_matrix.columns]\n",
        "\n",
        "# Ensure there are enough columns to plot a heatmap\n",
        "subset_corr = corr_matrix.loc[existing_columns_for_heatmap, existing_columns_for_heatmap]\n",
        "\n",
        "    # --- 7. Plot the Heatmap ---\n",
        "plt.figure(figsize=[60, 60]) # Increased size for more columns\n",
        "sns.heatmap(subset_corr, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=.9, annot_kws={\"size\": 40}) # Smaller annotation font\n",
        "plt.title(\"Correlation Matrix of Financial and Economic Indicators\", fontsize=45)\n",
        "plt.xticks(rotation=90, ha='right', fontsize=40) # Rotate and adjust font for readability\n",
        "plt.yticks(rotation=0, fontsize=40)\n",
        "plt.tight_layout()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h83FgwRosKot"
      },
      "source": [
        "# Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Crude_Price'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Crude_Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9mEzDCb14Bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Gold_Price'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Gold_Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MZsK2-vE4uhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Silver_Price'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Silver_Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S_LimTak4R9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Industrial_Production'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Industrial_Production')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zO1WnGZiPa6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Consumer_Price_Index'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Consumer Price Index')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JS8ps8SKPh7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Unemployment_Rate'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Unemployment_Rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_POw_ZZPq0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Retail_Sales'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Retail_Sales')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t4wx9VvsPyKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Producer_Price_Index'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Producer_Price_Index')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JVadLpVzP3e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Personal_Consumption_Expenditures'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Personal Consumption')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRbdSuIIQAzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['National_Home_Price_Index'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('National Home Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJMwDlbbQGKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['All_Employees(Total_Nonfarm)'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('All_Employees')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qmf5sOB_QS-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Labor_Force_Participation_Rate'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Labour Participation')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ut3vSEfKQaFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Money_Supply_(M2)'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Money Supply')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4n45z55zQuKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Trade_Balance'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Trade_Balance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c6V95taDQvLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Personal_Income'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Personal_Income')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3ivzjnbXQ31u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2, 2))\n",
        "plt.scatter(merged_df['Consumer_Sentiment'], merged_df['SP500_Price'], marker='o')\n",
        "plt.ylabel('SP500 Price')\n",
        "plt.xlabel('Consumer_Sentiment')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fm6s4PODQ-ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "KL93JY6kacdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sets can be modified by adding or subtracting desired features in the code where X is declared"
      ],
      "metadata": {
        "id": "Q9klwPlIkZPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmwJtrJuyQJS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8KsbpDXyXPr"
      },
      "outputs": [],
      "source": [
        "X = smallmerged_df[[\n",
        "'Crude_Price',\n",
        "'Gold_Price',\n",
        "'Silver_Price'\n",
        "\n",
        "]]\n",
        "y = smallmerged_df['SP500_NextPrice']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_test, y_pred)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7QJcocyKZG0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "siI-Gg-vaywP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sets can be modified by adding or subtracting desired features in the code where X is declared"
      ],
      "metadata": {
        "id": "VY0_jFKSknxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor,plot_tree\n",
        "\n",
        "X = smallmerged_df[[\n",
        "'Crude_Price',\n",
        "]]\n",
        "y = smallmerged_df['SP500_NextPrice']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "decisionmodel = DecisionTreeRegressor(max_depth=50)\n",
        "decisionmodel.fit(X_train, y_train)\n",
        "y_predd = decisionmodel.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_predd))\n",
        "print(\"R² Score:\", r2_score(y_test, y_predd))\n",
        "\n"
      ],
      "metadata": {
        "id": "tnY5NCJQcadr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-wBh3Me2OpQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plot_tree(decisionmodel, filled=True, feature_names=X_train.columns)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYkPvwczAI4"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_predd)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "xGMQ8XP7jJkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sets can be modified by adding or subtracting desired features in the code where X is declared"
      ],
      "metadata": {
        "id": "xp_ZYWIzkq3N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59dEn8Zky4Ko"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "X = smallmerged_df[[\n",
        "'National_Home_Price_Index',\n",
        "'Personal_Income',\n",
        "'Personal_Consumption_Expenditures'\n",
        "]]\n",
        "y = smallmerged_df['SP500_NextPrice']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "forestmodel = RandomForestRegressor(max_depth=30, random_state=1,n_estimators=5\n",
        ")\n",
        "forestmodel.fit(X_train, y_train)\n",
        "y_predf = forestmodel.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_predf))\n",
        "print(\"R² Score:\", r2_score(y_test, y_predf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG_4RF-01IBw"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_test, y_predf)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multilayer Perceptron Regressor\n"
      ],
      "metadata": {
        "id": "49zaFi5E0yWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sets can be modified by adding or subtracting desired features in the code where X is declared"
      ],
      "metadata": {
        "id": "gqB_OOuwk34Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = smallmerged_df[[\n",
        "\n",
        " 'Unemployment_Rate',\n",
        " 'Retail_Sales',\n",
        " 'Producer_Price_Index',\n",
        " 'Crude_Price',\n",
        "\n",
        "]]\n",
        "y = smallmerged_df['SP500_NextPrice']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")"
      ],
      "metadata": {
        "id": "rb4v8ANId_7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "MLP=MLPRegressor(hidden_layer_sizes=(128,64,32), activation='relu', solver='adam', max_iter=4000, random_state=1)\n",
        "MLP.fit(X_train, y_train)\n",
        "y_predm = MLP.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_predm))\n",
        "print(\"R² Score:\", r2_score(y_test, y_predm))"
      ],
      "metadata": {
        "id": "w_vWxKRUiSvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(y_test, y_predm)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PrZtwO_LwZ_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is used to test multiple combinations of structures for the same feature sets\n",
        "\n"
      ],
      "metadata": {
        "id": "8wESi4Gik59c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "values = [256, 128, 64, 32, 16, 8, 4,2]\n",
        "lists = list(combinations(values, 4))\n",
        "for lst in lists:\n",
        "  MLP=MLPRegressor(hidden_layer_sizes=lst, activation='relu', solver='adam', max_iter=4000, random_state=1)\n",
        "  MLP.fit(X_train, y_train)\n",
        "  y_predm = MLP.predict(X_test)\n",
        "  print(lst)\n",
        "  print(\"Mean Squared Error:\", mean_squared_error(y_test, y_predm))\n",
        "  print(\"R² Score:\", r2_score(y_test, y_predm))"
      ],
      "metadata": {
        "id": "h1Ed9k_txa2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recurrent Neural Network\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ThlC_minjnUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature sets can be modified by adding or subtracting desired features in the code where X is declared"
      ],
      "metadata": {
        "id": "zsLbulyHlZay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, callbacks, Sequential\n",
        "\n",
        "# ---- CONFIGURATION that can be tweaked ----\n",
        "LOOKBACK = 14\n",
        "HORIZON = 1\n",
        "VAL_RATIO = 0.15\n",
        "TEST_RATIO = 0.15\n",
        "UNITS = 32\n",
        "BATCH = 16\n",
        "EPOCHS = 200\n",
        "PATIENCE = 20\n",
        "\n",
        "\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "merged_df = merged_df.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "\n",
        "X = merged_df[[\n",
        "'National_Home_Price_Index',\n",
        "'Personal_Income',\n",
        "'Personal_Consumption_Expenditures'\n",
        "]]\n",
        "y = merged_df[['SP500_NextPrice']].astype(float).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "X_raw=X\n",
        "y_raw=y\n",
        "\n",
        "# 2) Scale features (and optionally y for stability)\n",
        "x_scaler = StandardScaler().fit(X_raw)\n",
        "y_scaler = StandardScaler().fit(y_raw)\n",
        "X_scaled = x_scaler.transform(X_raw)\n",
        "y_scaled = y_scaler.transform(y_raw)\n",
        "\n",
        "# 3) Build sliding windows: shape (num_samples, LOOKBACK, num_features)\n",
        "def make_windows(X, y, lookback=LOOKBACK, horizon=HORIZON):\n",
        "    Xw, yw = [], []\n",
        "    for i in range(len(X) - lookback - horizon + 1):\n",
        "        Xw.append(X[i:i+lookback])\n",
        "        yw.append(y[i+lookback+horizon-1])  # last point of horizon\n",
        "    return np.array(Xw), np.array(yw)\n",
        "\n",
        "Xw, yw = make_windows(X_scaled, y_scaled)\n",
        "\n",
        "# 4) Chronological split (no shuffling)\n",
        "n = len(Xw)\n",
        "n_test  = int(np.floor(TEST_RATIO * n))\n",
        "n_val   = int(np.floor(VAL_RATIO  * n))\n",
        "n_train = n - n_val - n_test\n",
        "\n",
        "X_train, y_train = Xw[:n_train], yw[:n_train]\n",
        "X_val,   y_val   = Xw[n_train:n_train+n_val], yw[n_train:n_train+n_val]\n",
        "X_test,  y_test  = Xw[n_train+n_val:],        yw[n_train+n_val:]\n",
        "\n",
        "# 5) tf.data pipelines (optional but nice)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH).prefetch(1)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH).prefetch(1)\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH)\n",
        "\n",
        "# 6) Define a tiny GRU model (swap to LSTM if you prefer)\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(LOOKBACK, X_train.shape[-1])),\n",
        "    layers.GRU(UNITS, dropout=0.2, recurrent_dropout=0),\n",
        "    # layers.GRU(64, return_sequences=True, dropout=0.2),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='mse')\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[es], verbose=0)\n",
        "\n",
        "# 7) Evaluate (invert scaling to meaningful units)\n",
        "y_pred_test_scaled = model.predict(test_ds).reshape(-1, 1)\n",
        "y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).ravel()\n",
        "\n",
        "# Align true test y and invert\n",
        "y_true_test = y_scaler.inverse_transform(y_test).ravel()\n",
        "\n",
        "mse = mean_squared_error(y_true_test, y_pred_test)\n",
        "r2   = r2_score(y_true_test, y_pred_test)\n",
        "\n",
        "print(f\"Test RMSE: {rmse:.3f}\")\n",
        "print(f\"Test MAE : {mae:.3f}\")\n",
        "print(f\"Test R^2 : {r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "AhT383YeLbcr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}